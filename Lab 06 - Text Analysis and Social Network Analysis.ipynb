{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Text Analysis and Social Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text analysis\n",
    "\n",
    "What we call \"text analysis\" in this class is often called *natural language processing* or *NLP* within computer science. NLP methods which enable computers to derive meaning from human language.\n",
    "\n",
    "A field has a lot of overlap with NLP is *machine learning* or *ML*. ML includes statistical methods that automatically detect patterns in data and used for making predictions in other data.\n",
    "\n",
    "The first part of this workshop on string manipulation will be NLP with some more basic Python functionality. The second part will focus on some ML examples of NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String manipulation\n",
    "\n",
    "Recall that the basic text unit in Python is the string. There's basic methods we can use with a string to get its length, to convert it to upper- and lowercase, to replace one substring with another, and to split into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_string = \"This is an example string. Strings are flexible.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(my_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_string.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## note that strings are case-sensitve\n",
    "my_string.replace(\"string\", \"piece of text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## string slice\n",
    "my_string[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_string.find(\"string\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically want to do these operations across a number of strings, and these are typically stored in data frames. Let's look at the first 20 tweets in the Canadian Tire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data/ct-example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a lot of the same kinds of string methods by using the set of <code>.str</code> methods in pandas. The complete list of these and a tutorial can be [found here](http://pandas.pydata.org/pandas-docs/stable/text.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df['text'].str.len().plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text'].str.replace('Canadian Tire', 'Canadian Fire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['created_at'].str.split(expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get the year-month pair\n",
    "df['date'] = df['created_at'].str.split(expand = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['month'] = df['date'].str.slice(0, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text'].str.find('Canadian Tire')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "The Reuters-21578 dataset is a set of Reuters business articles which is used as an example for text classification. \n",
    "We are going to use a reduced version of this set drawn from [here](http://ana.cachopo.org/datasets-for-single-label-text-categorization).\n",
    "\n",
    "1. Load these data with this command:\n",
    "    <code>df_r8 = pd.read_csv('data/r8-train-all-terms.txt', sep = \"\\t\", names = ['label', 'text'])</code>\n",
    "\n",
    "2. Identify the unique values of the column 'label'\n",
    "3. Take the length of all the text documents in the dataset. Store them in a column called 'length'.\n",
    "4. Split the text into separate words. Take the first word in the text and store it in the new column called 'first_word'.\n",
    "5. Identify all articles which mention the word 'trade'. Store them in a new data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "Now that we have used some basics of string handling, we need to know how to handle text for large-scale datasets. For that, text needs to go through several \"preprocessing\" steps before it can be passed to a statistical model.\n",
    "\n",
    "To do that, we will start to work with some of the tools included in [scikit-learn](http://scikit-learn.org/stable/). Most notably, we are going to 'vectorize' the text, which means we will convert the text from words to a numerical representation.\n",
    "\n",
    "There are three processes which we will perform in addition to vectorizing. One is removing all the *stopwords* from the text. Stopwords are words which appear very frequently in text and end up not adding much to our own subjective understanding of a string. Computationally, they appear often, which can also gum up statistical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second process is converting all of the words to lowercase. On a technical level, computers think that words which are the same but have different cases are different words. So we usually convert everything to lowercase. We already \n",
    "did that above, so we don't need to do another demostration of that.\n",
    "\n",
    "The third process is *tokenization*, meaning we separate all the meaningful *tokens* from each other. When we say tokens, we usually mean words. But tokens can also include certain kinds of punctuation which may be helpful to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(stop_words = 'english', lowercase = True)\n",
    "X    = vect.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers which are generated from the vectorization process are called *features*. Features get used to do other machine learning tasks, which we will cover below.\n",
    "\n",
    "Let's look at the features which are generated by this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the output of vectorization process. Each document is represented as a row in the matrix <code>X</code>, which is called a *term-document matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see which are the most used words in the list, we can take sum of all the words across all documents, then take the reverse order of words by their place in the list. Lastly, we use that ordering as an index to the <code>feature_names</code> list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totals = np.sum(X.toarray(), axis = 0)\n",
    "order  = np.argsort(totals)[::-1]\n",
    "feature_names[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often don't want to use simple counts for vectorization. Sometimes that will inflate the importance of words which are particular to the dataset. For instance, this dataset is guaranteed to have the words 'canadian tire' in every tweet. So we can use a metric called *term frequency - inverse document frequency* or [*tf-idf*](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect_tfidf = TfidfVectorizer(stop_words = 'english', lowercase = True)\n",
    "X_tfidf    = vect_tfidf.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "A major task of lots of NLP is labeling the content of a document. Twitter or Facebook, for instance, wants to classify whether a post might be relevant to you. A researcher might want to assess whether a policy document is more liberal or conservative. A brand might want to see if posts about them are positive or negative. This is where classification comes into view.\n",
    "\n",
    "The process of classifying text documents is depicted in the image below.\n",
    "![](img/supervised-learning.png)\n",
    "\n",
    "First, there are a set of documents which are labeled manually, i.e. by a human. The label is called a *class*. The dataset which is labeled manually is called the *training set*. It's called a training set because the machine learns from this set and then applies the knowledge it gets from the set to new, unseen data. The training is done on words or features which are part of documents. The particular statistical model which is trained is called a *classifier*. Then the body of documents which is to be classified by the classifier is called a *test set*. For the test set, the classes are hidden or unknown to the classifier. It is doing its best to guess the correct classes.\n",
    "\n",
    "There are a lot of different types of classifiers we can use for this task. But for this lab what we are going to use is a classifier called a *support vector machine* or SVM. In a very simiplified manner, what an SVM tries to do is draw an optimal hyperplane between a number of points in k-dimensional space, such that the points in the space are the furthest away from each other. So basically when the classifier sees new data, if it falls on one side of the plane, it will assign it the label associated with that side.\n",
    "\n",
    "![](img/hastie_etal-f12-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how do we actually know if the classifier did its job correctly. Well, usually, we have a test set in which we actually know the real labels. But we test those real labels against the predicted ones. We then develop a set of metrics called *precision* and *recall*, which assess two different things.\n",
    "\n",
    "![](img/precision-recall.png)\n",
    "\n",
    "Precision measures what percentage of the predicted items are relevant, while recall measures what percentage of the relevant items are predicted.\n",
    "\n",
    "Imagine this: you have a jar of coins. You want to go through the jar and pick out all the loonies and twoonies. One way of making sure you have all of the coins you want is to dump all the coins into your coin purse. In this case, your recall would be perfect (i.e. equal to 1) but your precision would be lousy. In the other case, you could search through the coins quickly with your hands and pick out which ever ones seem to pop out the quickest. You'll have much better precision here, but you might not get all the coins, so you would not have as good a recall.\n",
    "\n",
    "So let's get started. We're first going to load the modules needed for this. One is the SVM classifier, and the other two are assessment tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load both the training and test sets from the Reuters dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/r8-train-all-terms.txt', sep = \"\\t\", names = ['label', 'text'])\n",
    "df_test = pd.read_csv('data/r8-test-all-terms.txt', sep = \"\\t\", names = ['label', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we do is create a vectorizer for the words in the documents. We will load all the words for the training set into <code>X_train</code> and all the labels for the training set into <code>y_train</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect_tfidf = TfidfVectorizer(stop_words = 'english', lowercase = True)\n",
    "X_train = vect_tfidf.fit_transform(df_train['text'])\n",
    "y_train = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a similar thing for the test set. Notice how we use the method <code>transform</code> rather than <code>fit_transform</code>. That's because the vectorize is expecting a bunch of words which are defined only within the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vect_tfidf.transform(df_test['text'])\n",
    "y_test = df_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the classifier, and train it with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we predict the new labels, based on the words in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Network Analysis\n",
    "\n",
    "Social network analysis is a type of analysis which interprets, analyzes, and visualises *relational* data. Instead of beginning from the person or tweet as the unit of analysis, with social network analysis (or SNA) we begin from the relationship between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The building blocks of a network are *nodes* and *edges*. Nodes represent individuals in the network. They are people, tweets, firms, Twitter users, etc. They are the thing doing the interaction.\n",
    "\n",
    "![](img/net-1-node.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection between nodes are called *edges*. They imply some kind of relationship between the edges. This interaction could be friendship, mutual attendance of an event, dating, or has done business with.\n",
    "\n",
    "![](img/net-1-edge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edges can be *directed* or *undirected*. For instance, on Facebook, friendships are mutual and both parties must agree to that friendship. Therefore, it is called *undirected* because it is by definition a two-way relationship. However, on Twitter, user A can follow user B, but user B does not have to follow user A. This is called a *directed* graph because it can be a one-way relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, edges can be *weighted*. Weights are usually numerical values which indicate a strength of a relationship. The edge between you and your best friend is probably higher than you and one of your classmates who you do not speak to often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://evelinag.com/blog/2015/12-15-star-wars-social-network/star-wars-logo.png)\n",
    "\n",
    "In this lab we will be using a small network that indicates [interactions in the movie Star Wars Episode IV](http://evelinag.com/blog/2015/12-15-star-wars-social-network/). Here, each node is a character and each edge indicates whether they appeared together in a scene of the movie. Edges here are thus undirected and they also have weights attached, since they can appear in multiple scenes together.\n",
    "\n",
    "The first step is to read the list of edges in this network. For this exercise, we are going to use the <code>[networkx](https://networkx.github.io/)</code> module to read, analyse, and visualise the networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the <code>matplotlib</code> module for visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read in the network as a weighted edgelist. This is a CSV file in the format of <code>node1, node2, weight</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.read_weighted_edgelist('data/star-wars-network-edges.csv', delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a method to see all the edges in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use a similar one to see all the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a specific attribute of an edge, we need to use <code>get_edge_attributes</code>. Who seems to have the highest weight in their interactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx.get_edge_attributes(G, 'weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to draw. We'll use the basic <code>draw</code> method first to illustrate the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks interesting, but we don't really know which node is which unless we add some labels. Before we add labels, we need to assign the labels particular positions on the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to play with two layouts. The first is a \"circular\" layout, which is useful because we can see all the nodes and the connections between them. However, with this layout, we have a harder time seeing what groups of nodes seem to cluster together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second layout is called \"Fruchterman-Reingold\". It is a \"force-directed\" layout, which implies that if subnetworks seem to be tied closer together, they squeeze together more in the graph. Let's play with both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = nx.fruchterman_reingold_layout(G)\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "nx.draw(G, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're starting to see some patterns, even if we can't really see much of the text. Peripherial characters like Jabba and Greedo are only connected by one edge. However, in the center there seems to be a cluster of people like Luke, R2-D2, and Chewie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing with a circular layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = nx.circular_layout(G)\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "nx.draw(G, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lets us see more of the links that exist between different nodes. It's actually not super useful, though, unless we have some more information about the edges. That's where the weights come into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the weight of the edge. We can do this by setting some levels for line weights. We can have three: small, mid, and large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## select edges by weight\n",
    "esmall = [(u,v) for (u,v,d) in G.edges(data = True) if d['weight']  < 5]\n",
    "emid   = [(u,v) for (u,v,d) in G.edges(data = True) if d['weight'] >= 5 and d['weight'] < 10 ]\n",
    "elarge = [(u,v) for (u,v,d) in G.edges(data = True) if d['weight'] >= 10]\n",
    "\n",
    "## draw edges in varying edge widths\n",
    "nx.draw_networkx_edges(G, pos, edgelist = elarge, width = 4, alpha = 0.5)\n",
    "nx.draw_networkx_edges(G, pos, edgelist = emid,   width = 2, alpha = 0.5)\n",
    "nx.draw_networkx_edges(G, pos, edgelist = esmall, width = 1, alpha = 0.5)\n",
    "nx.draw_networkx_nodes(G, pos)\n",
    "\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can see some stronger links between people like Chewie and Han, Luke and Obi-Wan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can set the colours of nodes based on whether the person is on the light side, the dark side, or is other. Let's use the Fruchterman-Reingold layout because it allows us to see clusters a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## select nodes by light side / dark side / other\n",
    "dark_side = [\"DARTH VADER\", \"MOTTI\", \"TARKIN\"]\n",
    "light_side = [\"R2-D2\", \"CHEWBACCA\", \"C-3PO\", \"LUKE\", \"CAMIE\", \"BIGGS\",\n",
    "                \"LEIA\", \"BERU\", \"OWEN\", \"OBI-WAN\", \"HAN\", \"DODONNA\",\n",
    "                \"GOLD LEADER\", \"WEDGE\", \"RED LEADER\", \"RED TEN\"]\n",
    "other = [\"GREEDO\", \"JABBA\"]\n",
    "\n",
    "pos = nx.fruchterman_reingold_layout(G)\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, edgelist = elarge, width = 4, alpha = 0.5)\n",
    "nx.draw_networkx_edges(G, pos, edgelist = emid,   width = 2, alpha = 0.5)\n",
    "nx.draw_networkx_edges(G, pos, edgelist = esmall, width = 1, alpha = 0.5)\n",
    "\n",
    "## draw the nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_color = 'red', nodelist = dark_side)\n",
    "nx.draw_networkx_nodes(G, pos, node_color = 'yellow', nodelist = light_side)\n",
    "nx.draw_networkx_nodes(G, pos, node_color = 'gray', nodelist = other)\n",
    "nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some clear patterns here. The light side is very much clustered together, while the dark side has its own grouping. The outliers -- Jabba and Greedo -- aren't grouped at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to graphing, we can create some network-level statistics which characterize the network. This includes *density*, which measures how many of the possible connections in this network have been made. If density equals 1, that would imply that everyone in the movie had a scene with everyone else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx.density(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, there are node-level statistics which characterize individual nodes. One of the more important one of these is *degree*, which means how many edges are connected to this particular node. Which nodes seem to have the highest degree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx.degree(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
